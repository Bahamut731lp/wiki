# Informační entropie
Entropie [diskrétní náhodné veličiny](../matematika/statistika/teorie_pravdepodobnosti/nahodna_velicina.md) $X$ je jednotka udávající očekáváné množství informace potřebné k popisu proměnné.

$$
H(X) = -\sum_{x \in X} p(x) \cdot \log{p(x)} = \sum_{x \in X} p(x) \cdot \log{\frac{1}{p(x)}}
$$

!!! bug ""
    https://www.youtube.com/watch?v=YtebGVx-Fxw

## Míra překvapení

## Entropie
